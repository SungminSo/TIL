# 토스의 서버 인프라 모니터링

## 토스 서버 인프라의 변화
- 토스 마이크로 서비스 규모의 변화
	- 2019년 마이크로 서비스 130개 -> 2021년 마이크로 서비스 236개
	- 기본적으로 컨테이너 기반의 서버 오케스트레이션 시스템을 서버 인프라로 사용
	- 2019년 중반까지는 VAMP, DC/OS 기반의 인프라 운영
	- DC/OS : 컨테이너 오케스트레이션 엔진
	- VAMP : 마이크로 서비스에서 서비스 디스커버리를 담당하는 API Gateway 컴포넌트
	- 인프라 변화의 이유 : DC/OS와 VAMP의 운영 코스트가 높았고, 서비스 디스커버리의 한계를 경험했기 때문
	- kubernetes/Istio를 적용 -> 대규모에 맞게 설계되어 있어 문제 해결에 도움이 될 것으로 판단 -> 마이그레이션 진행 -> 9개월에 걸쳐 적용 완료
	- 마이그레이션에서 겪는 어려움은 신규 시스템이 기존 시스템과 달리 또 다른 문제를 만드는 것
	- 때문에 신규 인프라의 가시성을 빠르게 프로덕션 레벨로 올려야했고, 그 과정에서 모니터링 시스템도 변화하게 되었음
     
## 3가지 레이어의 모니터링
- 기존에는 influxdb와 telegraf로 모니터링
- Istio/k8s로 마이그레이션 하면서 k8s 생태계를 적극적으로 활용하다 보니 프로메테우스가 점차 메인 모니터링 시스템으로 자리 잡게 됨
- 보통 오류로그를 발견해 서비스 이슈 발생 여부를 알게 되는데, 로직이 복잡하거나 트랜잭션이 길 경우 파악이 어려움
- 또 잘 동작하다가 갑자기 오류가 나는 이슈가 까다롭고 꽤 자주 겪게 되는 이슈인데 이 경우 메트릭이 원인 규명에 도움이 됨
	- Application Layer Metric
		- 언어환경, 프레임워크 관련 메트릭
		- 오류와 가장 상관관계가 높은 단계의 메트릭이기 때문에 오류 로그를 살필때 1순위
	- Network Layer Metric
		- 서비스와 서비스 간의 통신 메트릭
		- 모든 네트워크 퍼널의 가시성 확보
	- OS Layer Metric
		- OS 리소스 관련 지표들
		- 오류와 가장 상관관계가 낮음
     
- Network Layer Metric
	- 서비스 메쉬로 인프라가 변경되면서 네트워크 제어권 강화됨
	- 서비스 간 통신에서 어떤 버전의 source 어플리케이션이 destination 어플리케이션의 어떤 버전으로 어떤 요청을 넣었는지 알 수 있게 됨
	- `reporter`라는 속성으로 source와 destination이 다른 응답으로 처리되는 경우도 캐치할 수 있게 됨
	- `response_flags`라는 지표로 하나의 요청 flow에서 upstream, downstream 사이에서 어떤 비정상적 현상이 있었는지 파악하는데 도움이 됨
	- ex. `destination_app / destination_version / response_flags / source_app / source_version / reporter / host / request_protocol / response_code / url`
	- 서비스 디스커버리 이슈
		- UH (No healthy upsream hosts)
		- UO (Upstream overflow)
		- NR (No route configured)
	- 요청 커넥션 이슈
		- DC (Downstream connection termination)
		- UC (Upstream connection termination)
		- UT (Upstream request timeout)
		- UF (Upstream connection failure)
		- URX (The request was rejected because the upstream retry limit)
     
- OS Layer Metric
	- USE method : 넷플릭스 퍼포먼스 엔지니어가 만든 분석기법으로, 각 하드웨어 리소스에서 에러가 있는지, 사용률이 높은지, 리소스가 포화되어 더 이상 못쓰는 상태인지 등을 보고 각 리소스의 문제를 배제해 나가는 방식
	
## 모니터링 인프라의 운영 개선
- 장비의 문제
	- 하드웨어 문제, 휴먼에러 등으로 서버 머신이 내려간 경우 빠르게 해당 인스턴스의 타겟을 제거해야 함
	- 모든 머신에 1초마다 ping을 날려서 머신의 상태 감지
	- 특정 threshold를 넘기면 해당 머신의 어플리케이션 제거
- 모니터링 문제
	- 모니터링 인프라에서 문제가 발생하면 메트릭을 볼 수 없는 문제 발생
	- 프로메테우스의 경우 어플리케이션의 메트릭을 스크랩해가는 방식이기 때문에 죽으면 메트릭 수집이 중단되었고, 당시의 메트릭은 누락되게 됨
	- 프로메테우스 메모리 사용량은 수집되는 메트릭량과 저장하는 포멧인 tsdb가 로드되는 양과 비례
	- 메모리 과다 사용시 프로메테우스가 `out of memory` 이슈가 발생해 내려가는 문제가 있음
	- 해결 방법은 수집하는 메트릭을 최적화하고, 스케일아웃 방법 사용(hashmod 옵션으로 샤딩처리), 타노스 쿼리를 통해 통합 쿼리 사용