# 토스 데이터의 흐름과 활용
      
## 데이터 정의
- 클라이언트 개발자들과 협의하여 필요한 로그들은 SDK를 이용해 추출
- log centre : 로그 정의 시스템     
	- 화면에 대한 로그(화면 캡쳐된 형식의 로그 + 해당 화면에서 발생하는 이벤트 정의)     
	- 로그에 이상이 있을 경우(valid 체크 시 fail이 났을 경우) slack 알림
         
## 데이터 수집 및 저장
- Mysql DB 데이터 -> Sqoop -> HDFS
- Mongo DB 데이터 -> hive mongo storage handler -> HDFS
- batch centre : DB 데이터를 HDFS에 쉽게 적재하기 위한 툴  
	- 접근 DB, 테이블 선택 -> 등록 -> HDFS 적재 스케줄 등록(이후 주기적으로 적재하게 됨)
	- 적재된 데이터는 Hadoop Ecosystem에서 Apache Sentry를 이용해 인증권한관리
	- 등록된 작업은 Jenkins와 연동되어 관리
  
- 앱, 서버로그 -> Kafka -> HDFS / streaming 처리
	- ex) 앱 -> log collect API -> Kafka(app_event) -> streaming -> Kafka(app_event_v2: 로그 센터에 정의된 스키마와 비교하여 valid 체크) -> HDFS(impala)
	- stream-connect(토스 자체 개발) -> HBase / Dudu / HDFS
 
- Hadoop에 적재된 데이터는 데이터 노드 스토리지 영역 구분(HDFS의 Heterogeneous Storage 기능 사용)
	- 도입 이유 : 조회가 빈번하지 않은 보관 영역 size가 점차 누적되어 늘어남 & 실시간 서비스를 위한 영역은 대용량 ETL처리에 의한 IO 간섭 발생
	- HOT -> SSD
	- WARM -> DISK : hadoop3부터 지원하는 erasure coding으로 일반적인 데이터 저장 -> impala, hive, spark 등 ETL 처리에 이용
	- COLD -> NAS : 백업이나 장기간 보관이 필요한 로그 저장
       
## 데이터 추출, 가공, 적재(ETL)
- impala, hive, spark를 이용하여 처리, airflow, jenkins를 이용하여 플로우 관리
- SQL의 제한적인 부분 또는 복잡도 높은 처리 -> spark 또는 hive 사용
- 이외에는 주로 impala 사용
	- multi-user performance와 query throughput 때문
	- coordinators와 executors로 역할 분리 가능
		- coordinators : JDBC/ODBC 커넥션으로 쿼리 listen 역할도 수행
	- 단점 : 
		- graceful shutdown와 자체적인 쿼리 retry 제공 안됨
			- graceful shutdown은 impala 3.1부터 지원
			- retry는 airflow나 jenkins로 처리 
		- 노드 간 메타 데이터 sync 필요
			- SYNC_DDL 옵션으로 처리가능
		- spark, hive 등 처리 시 impala로 메타정보가 반영되지 않음
			- workflow에서 처리 후 invalidate metadata처리를 통해 캐시 flush -> 이후 쿼리 시 메타정보를 새로 불러오게됨 
		- catalog, StateStore는 HA 구성을 지원하지 않아 SPOF(Single Point of Failure)
			- EDA, ETL 용도의 두개의 클러스터 분리 이후 active-active 구조로 운영 -> HAProxy를 통해 한쪽 클러스터로만 접근가능하도록 함
		- 쿼리플랜을 잡을 때 예상되는 메모리가 가용량을 초과할 경우 쿼리를 reject하기 때문에 실제 할당 영역보다 maximum을 크게 줘야 함
		- 메트릭 모니터링 시 메모리 초과 시 모든 쿼리가 실패하기도 함 
			- 각 코디네이터에서 제공하는 쿼리 프로파일 수집 -> kafka로 적재 -> druid supervisor를 활용해 grafana에서 모니터링
			- 여기서 long run 쿼리나 전체 클러스터 사용 리소스가 과다한 경우 선제 차단하도록 처리
			- grafana 대시보드에는 쿼리 히스토리와 리소스 모니터링
        
- 간단한 배치 작업 : batch centre + jenkins 사용
- task 간에 dependency가 복잡한 주요 ETL 파이프라인은 airflow 사용
      
- 리얼타임 프로세싱 
	- 스트리밍처리 : Flink, Spark streaming, Druid, 토스 자체 개발 스트리밍 커넥트 
	- 실시간 데이터 적재 및 서빙 : influxDB, Redis, HBase, Hadoop, Kudu, ES, Druid
	- 실시간 쿼리 엔진 : Druid, ES, InfluxDB, Impala + Kudu
   
- HDFS, KUDU, Impala 활용 사례(lambda architecture)
	- impala가 아닌 다른 컴포넌트를 통해 HDFS에 실시간 적재되는 데이터는 메타데이터 실시간 sync에 어려움이 있음(매번 refresh 또는 invalidate metadata 처리)
	- KUDU 클러스터는 HDFS 클러스터에 비해 규모가 작아서 kudu만으로 모든 데이터를 보관하기에는 어려움(데이터 size가 비효율적)
		- daily partitioning 하여 어제자 partition까지는 HDFS 기반 테이블 조회, 오늘자 데이터는 KUDU 조회하도록 처리
		- 두 테이블 모두 실시간으로 데이터 적재, kudu에는 당일 데이터만 보관, partition_day를 당일 날짜 기준으로 union view table 생성
      
## 데이터 분석(EDA)
- Jupyter나 Hue를 통해 spark, hive, impala에 접근해서 데이터 분석
     
## 데이터 모니터링
- k8s에 올라간 app들은 prometheus를 통해, 기타 하둡 에코시스템이나 서비스 메트릭들은 Druid, InfluxDB, Elasticsearch 등의 datasource로 Kibana나 Grafana로 모니터링하고 slack으로 알림받음
     
## 데이터 활용
- 실시간 데이터:
	- 스트리밍 처리 -> Redis, HBase, MongoDB에 적재되는 데이터 활용
	- batch 데이터 -> MySQL, MongoDB 주기적으로 적재 
 
- 자유롭고 쉬운 분석을 위해 자체 툴 개발 : TUBA, Toss Analytics
 
- ML 모델 예측으로 CTR이 높은 유저를 타깃팅해서 마케팅 메세지를 보내는 기능도 있음
