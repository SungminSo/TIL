# HyperCLOVA를 위한 Big Data

딥러닝에서 중요한 부분 : 좋은 데이터





HypyerCLOVA 대용량 데이터를 준비하며 고민한 내용

1. 다양한 내용
   - 어떤 하나의 유형이나 내용으로 치우치지 않아야 함
   - 일상에서 접할 수 있는 다양한 정보의 내용과 모습이어야 함
2. 범용의 구성
   - 목표로 하는 검색, 대화, Q&A, 요약 등 여러 생성 작업을 포함해야 함
3. 양질의 정보
   - 내용이 믿고 신뢰할 수 있어야 함
4. 충분한 크기
   - 목표로 하는 모델의 크기를 구축하는데 있어서 충분한 양이어야 함

​                 

다양한 내용

- 기반 지식
  - 정보의 범용성과 완결성을 고려해서 객관적 사실 중심의 다양한 풀을 갖추려 함
- 검색 허용된 문서
  - 품질 순으로 오더링하여 차례대로 가져옴
- 신뢰할 수 있는 출처의 오픈된 리소스
  - ex) 모두의 말뭉치
- 전문 지식
  - 정보의 다양성 확보
- 중복 내용은 제거
  - 한쪽 내용으로 치우치지 않도록 함
- 혹시나 포함될지 모르는 개인 정보는 제거하거나 비식별화

​      

범용의 구성

- 데이터의 구조를 유의미하게 재배치
  - ex) 지식인 -> 기존 : 질문 + 답변 + 답변 + ... / 재배치 : 질문 + 채택 답변
- 문서의 양식은 유지하도록
  - 화면에 보이는 모습을 유지하도록 html 파싱
- 메타정보 추가
  - 대화문에서는 동일 화자 구분을 위해 화자 ID 추가
  - 문서에 카페명 등 출처, 카테고리 데이터 추가

​     

양질의 정보

- 서비스 로그를 통해 어느 정보가 유용한지, 인기가 있는지 파악하여 해당 출처가 상위품질에 많이 포함되도록 데이터 선별
- 웹 페이지 내 핵심영역만 사용하여 데이터 수집
  - 정보 가치가 일정 수준 이상인 부분만 찾아내는 모델을 생성하여 사용
- 저품질 문서 필터링
  - 의미 없는 단어의 나열
  - 비속어나 유해 정보 제거